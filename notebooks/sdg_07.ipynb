{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"../sdg/data/retail-data/all/*.csv\")\\\n",
    "    .coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: int, Country: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "\n",
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, mean\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    mean(\"Quantity\").alias(\"mean_purchases\")\n",
    ").selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Wikipedia:*\n",
    "> In statistics, mean, median, and mode are all known as measures of central tendency, and in colloquial usage any of these might be called an average value.\n",
    "\n",
    "The term *mean* also refers to *expected value*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+---------------------+\n",
      "| var_pop(Quantity)|stddev_pop(Quantity)|var_samp(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+--------------------+------------------+---------------------+\n",
      "|47559.303646609056|  218.08095663447796|47559.391409298754|   218.08115785023418|\n",
      "+------------------+--------------------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "\n",
    "df.select(var_pop(\"Quantity\"), stddev_pop(\"Quantity\"), var_samp(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052562|119768.05495536952|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|     4.912186085635685E-4|            1052.7260778741693|             1052.7280543902734|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "\n",
    "df.select(\n",
    "    corr(\"InvoiceNo\", \"Quantity\"),\n",
    "    covar_pop(\"InvoiceNo\", \"Quantity\"),\n",
    "    covar_samp(\"InvoiceNo\", \"Quantity\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   536596|      null|    6|\n",
      "|   537252|      null|    1|\n",
      "|   538041|      null|    1|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "|   537252|   1|              1|\n",
      "|   537691|  20|             20|\n",
      "|   538041|   1|              1|\n",
      "|   538184|  26|             26|\n",
      "|   538517|  53|             53|\n",
      "|   538879|  19|             19|\n",
      "|   539275|   6|              6|\n",
      "|   539630|  12|             12|\n",
      "|   540499|  24|             24|\n",
      "|   540540|  22|             22|\n",
      "|  C540850|   1|              1|\n",
      "|   540976|  48|             48|\n",
      "|   541432|   4|              4|\n",
      "|   541518| 101|            101|\n",
      "|   541783|  35|             35|\n",
      "|   542026|   9|              9|\n",
      "|   542375|   6|              6|\n",
      "|  C542604|   8|              8|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+\n",
      "|InvoiceNo|    avg(UnitPrice)|     avg(Quantity)|\n",
      "+---------+------------------+------------------+\n",
      "|   536596| 5.723333333333334|               1.5|\n",
      "|   536938|3.2885714285714287|33.142857142857146|\n",
      "|   537252|              0.85|              31.0|\n",
      "|   537691| 3.282000000000001|              8.15|\n",
      "|   538041|               0.0|              30.0|\n",
      "|   538184| 2.461538461538462|12.076923076923077|\n",
      "|   538517| 2.855849056603773|3.0377358490566038|\n",
      "|   538879| 1.184736842105263|21.157894736842106|\n",
      "|   539275|3.0833333333333335|              26.0|\n",
      "|   539630| 3.391666666666667|20.333333333333332|\n",
      "|   540499|           5.53125|              3.75|\n",
      "|   540540| 3.809090909090909|2.1363636363636362|\n",
      "|  C540850|              1.25|              -1.0|\n",
      "|   540976|2.8972916666666673|10.520833333333334|\n",
      "|   541432|             3.325|             12.25|\n",
      "|   541518|2.2398019801980196| 23.10891089108911|\n",
      "|   541783| 4.036571428571427|11.314285714285715|\n",
      "|   542026| 4.427777777777778| 7.666666666666667|\n",
      "|   542375| 5.766666666666666|               8.0|\n",
      "|  C542604|            7.7775|              -8.0|\n",
      "+---------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregations = {\"Quantity\": \"avg\", \"UnitPrice\": \"avg\"}\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(aggregations).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "\n",
    "Spark supports three kinds of window functions:\n",
    "- ranking functions\n",
    "- analytic functions\n",
    "- aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "windowSpec = Window.partitionBy(\"CustomerId\", \"date\")\\\n",
    "    .orderBy(desc(\"Quantity\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank\n",
    "\n",
    "purchaseDensRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|              74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                 36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                 36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    "    .select(\n",
    "        col(\"CustomerId\"),\n",
    "        col(\"date\"),\n",
    "        col(\"Quantity\"),\n",
    "        purchaseRank.alias(\"quantityRank\"),\n",
    "        purchaseDensRank.alias(\"quantityDenseRank\"),\n",
    "        maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull.createOrReplaceTempView(\"dfNotNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|2010-12-01|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|2010-12-01|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|2010-12-01|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNoNull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|StockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "|     18287|   85039A|           96|\n",
      "|     18287|    84920|            4|\n",
      "|     18287|    84584|            6|\n",
      "|     18287|   84507C|            6|\n",
      "|     18287|   72351B|           24|\n",
      "|     18287|   72351A|           24|\n",
      "|     18287|   72349B|           60|\n",
      "|     18287|    47422|           24|\n",
      "|     18287|    47421|           48|\n",
      "|     18287|    35967|           36|\n",
      "|     18287|    23445|           20|\n",
      "|     18287|    23378|           24|\n",
      "|     18287|    23376|           48|\n",
      "|     18287|    23310|           36|\n",
      "|     18287|    23274|           12|\n",
      "|     18287|    23272|           12|\n",
      "|     18287|    23269|           36|\n",
      "+----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, StockCode, sum(Quantity) FROM dfNotNull\n",
    "GROUP BY CustomerId, StockCode\n",
    "ORDER BY CustomerId DESC, StockCode DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Grouping sets depend on null values for aggregation levels. If you do not filter-out null values, you will get incorrect results. This applies to cubes, rollups, and grouping sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|StockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|      null|     null|      4906888|\n",
      "|     18287|     null|         1586|\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "|     18287|   85039A|           96|\n",
      "|     18287|    84920|            4|\n",
      "|     18287|    84584|            6|\n",
      "|     18287|   84507C|            6|\n",
      "|     18287|   72351B|           24|\n",
      "|     18287|   72351A|           24|\n",
      "|     18287|   72349B|           60|\n",
      "|     18287|    47422|           24|\n",
      "|     18287|    47421|           48|\n",
      "|     18287|    35967|           36|\n",
      "|     18287|    23445|           20|\n",
      "|     18287|    23378|           24|\n",
      "|     18287|    23376|           48|\n",
      "|     18287|    23310|           36|\n",
      "|     18287|    23274|           12|\n",
      "+----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT customerId, StockCode, sum(Quantity) FROM dfNotNull\n",
    "GROUP BY CustomerId, StockCode GROUPING SETS ((CustomerId, StockCode), (CustomerId), ())\n",
    "ORDER BY CustomerId DESC NULLS FIRST, StockCode DESC NULLS FIRST\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      null|          null|       4906888|\n",
      "|2010-12-01|          null|         24032|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|United Kingdom|         21167|\n",
      "|2010-12-02|          null|         20855|\n",
      "|2010-12-02|          EIRE|             4|\n",
      "|2010-12-02|       Germany|           146|\n",
      "|2010-12-02|United Kingdom|         20705|\n",
      "|2010-12-03|          null|         11548|\n",
      "|2010-12-03|       Belgium|           528|\n",
      "|2010-12-03|          EIRE|          2375|\n",
      "|2010-12-03|        France|           239|\n",
      "|2010-12-03|       Germany|           170|\n",
      "|2010-12-03|         Italy|           164|\n",
      "|2010-12-03|        Poland|           140|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpFD = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    "    .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\\\n",
    "    .orderBy(\"Date\", \"Country\")\n",
    "\n",
    "rolledUpFD.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+--------------+\n",
      "|Date|           Country|total_quantity|\n",
      "+----+------------------+--------------+\n",
      "|null|              null|       4906888|\n",
      "|null|         Australia|         83653|\n",
      "|null|           Austria|          4827|\n",
      "|null|           Bahrain|           260|\n",
      "|null|           Belgium|         23152|\n",
      "|null|            Brazil|           356|\n",
      "|null|            Canada|          2763|\n",
      "|null|   Channel Islands|          9479|\n",
      "|null|            Cyprus|          6317|\n",
      "|null|    Czech Republic|           592|\n",
      "|null|           Denmark|          8188|\n",
      "|null|              EIRE|        136329|\n",
      "|null|European Community|           497|\n",
      "|null|           Finland|         10666|\n",
      "|null|            France|        109848|\n",
      "|null|           Germany|        117448|\n",
      "|null|            Greece|          1556|\n",
      "|null|           Iceland|          2458|\n",
      "|null|            Israel|          3990|\n",
      "|null|             Italy|          7999|\n",
      "+----+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(\"Quantity\").alias(\"total_quantity\"))\\\n",
    "    .select(\"Date\", \"Country\", \"total_quantity\")\\\n",
    "    .orderBy(\"Date\", \"Country\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import grouping_id\n",
    "\n",
    "dfCubeWithGroupingId = dfNoNull.cube(\"CustomerId\", \"StockCode\")\\\n",
    "    .agg(grouping_id(), sum(\"Quantity\").alias(\"total_quantity\"))\\\n",
    "    .orderBy(grouping_id().desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+--------------+\n",
      "|CustomerId|StockCode|grouping_id()|total_quantity|\n",
      "+----------+---------+-------------+--------------+\n",
      "|      null|     null|            3|       4906888|\n",
      "|      null|    22427|            2|          1954|\n",
      "|      null|    22468|            2|           464|\n",
      "|      null|    22473|            2|           437|\n",
      "|      null|    85064|            2|           203|\n",
      "|      null|   90123A|            2|            14|\n",
      "|      null|    22442|            2|           281|\n",
      "|      null|    22154|            2|          3525|\n",
      "|      null|    21446|            2|           400|\n",
      "|      null|    20697|            2|            19|\n",
      "|      null|    22946|            2|           353|\n",
      "|      null|    16015|            2|           662|\n",
      "|      null|   90003D|            2|            14|\n",
      "|      null|   85194S|            2|          1134|\n",
      "|      null|   85130C|            2|           479|\n",
      "|      null|   90183C|            2|             1|\n",
      "|      null|    23208|            2|          6152|\n",
      "|      null|    23110|            2|           613|\n",
      "|      null|    23020|            2|           170|\n",
      "|      null|   90205A|            2|            16|\n",
      "+----------+---------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCubeWithGroupingId.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+--------------+\n",
      "|CustomerId|StockCode|grouping_id()|total_quantity|\n",
      "+----------+---------+-------------+--------------+\n",
      "|      null|     null|            3|       4906888|\n",
      "+----------+---------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCubeWithGroupingId.where(grouping_id() == 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+--------------+\n",
      "|CustomerId|StockCode|grouping_id()|total_quantity|\n",
      "+----------+---------+-------------+--------------+\n",
      "|      null|    22427|            2|          1954|\n",
      "|      null|    22867|            2|          5948|\n",
      "|      null|    22350|            2|          1355|\n",
      "|      null|    21413|            2|            36|\n",
      "|      null|    22683|            2|           563|\n",
      "|      null|    21736|            2|            77|\n",
      "|      null|   84927A|            2|            45|\n",
      "|      null|   90123B|            2|             8|\n",
      "|      null|    84748|            2|            28|\n",
      "|      null|    21371|            2|           275|\n",
      "|      null|   84750A|            2|            -3|\n",
      "|      null|    23545|            2|          2475|\n",
      "|      null|    23505|            2|          1065|\n",
      "|      null|    23531|            2|           963|\n",
      "|      null|    20685|            2|          3835|\n",
      "|      null|   72800B|            2|           140|\n",
      "|      null|    22277|            2|           355|\n",
      "|      null|   85135B|            2|            46|\n",
      "|      null|   90178A|            2|             6|\n",
      "|      null|    21507|            2|          3182|\n",
      "+----------+---------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCubeWithGroupingId.where(grouping_id() == 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+--------------+\n",
      "|CustomerId|StockCode|grouping_id()|total_quantity|\n",
      "+----------+---------+-------------+--------------+\n",
      "|     16752|     null|            1|            54|\n",
      "|     14083|     null|            1|           732|\n",
      "|     15716|     null|            1|           744|\n",
      "|     14524|     null|            1|           676|\n",
      "|     13124|     null|            1|          2147|\n",
      "|     17001|     null|            1|          2164|\n",
      "|     17392|     null|            1|           320|\n",
      "|     13693|     null|            1|            -6|\n",
      "|     15709|     null|            1|           121|\n",
      "|     15669|     null|            1|           339|\n",
      "|     16226|     null|            1|           192|\n",
      "|     15152|     null|            1|          2694|\n",
      "|     16796|     null|            1|           186|\n",
      "|     15466|     null|            1|           246|\n",
      "|     18278|     null|            1|            66|\n",
      "|     14121|     null|            1|          2078|\n",
      "|     16793|     null|            1|           146|\n",
      "|     13897|     null|            1|           200|\n",
      "|     13017|     null|            1|            48|\n",
      "|     16583|     null|            1|           111|\n",
      "+----------+---------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCubeWithGroupingId.where(grouping_id() == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+--------------+\n",
      "|CustomerId|StockCode|grouping_id()|total_quantity|\n",
      "+----------+---------+-------------+--------------+\n",
      "|     16029|    21731|            0|           984|\n",
      "|     12431|    22726|            0|            28|\n",
      "|     15862|    22435|            0|             2|\n",
      "|     14307|    21705|            0|            12|\n",
      "|     17908|    21326|            0|            12|\n",
      "|     14849|    21586|            0|            12|\n",
      "|     17377|    22630|            0|            16|\n",
      "|     12472|    22333|            0|            14|\n",
      "|     15694|    20749|            0|            72|\n",
      "|     13715|    21172|            0|            12|\n",
      "|     17017|    22555|            0|            72|\n",
      "|     13117|    84945|            0|            24|\n",
      "|     16916|    22505|            0|             1|\n",
      "|     16916|    22558|            0|             2|\n",
      "|     15061|    22075|            0|           312|\n",
      "|     15574|    22087|            0|             2|\n",
      "|     17757|    22867|            0|            16|\n",
      "|     18239|    20749|            0|             2|\n",
      "|     18239|    22065|            0|            12|\n",
      "|     18168|   16156S|            0|            25|\n",
      "+----------+---------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCubeWithGroupingId.where(grouping_id() == 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'Australia_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Australia_sum(UnitPrice)',\n",
       " 'Australia_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Austria_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Austria_sum(UnitPrice)',\n",
       " 'Austria_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Bahrain_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Bahrain_sum(UnitPrice)',\n",
       " 'Bahrain_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Belgium_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Belgium_sum(UnitPrice)',\n",
       " 'Belgium_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Brazil_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Brazil_sum(UnitPrice)',\n",
       " 'Brazil_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Canada_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Canada_sum(UnitPrice)',\n",
       " 'Canada_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Channel Islands_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Channel Islands_sum(UnitPrice)',\n",
       " 'Channel Islands_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Cyprus_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Cyprus_sum(UnitPrice)',\n",
       " 'Cyprus_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Czech Republic_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Czech Republic_sum(UnitPrice)',\n",
       " 'Czech Republic_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Denmark_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Denmark_sum(UnitPrice)',\n",
       " 'Denmark_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'EIRE_sum(CAST(Quantity AS BIGINT))',\n",
       " 'EIRE_sum(UnitPrice)',\n",
       " 'EIRE_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'European Community_sum(CAST(Quantity AS BIGINT))',\n",
       " 'European Community_sum(UnitPrice)',\n",
       " 'European Community_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Finland_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Finland_sum(UnitPrice)',\n",
       " 'Finland_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'France_sum(CAST(Quantity AS BIGINT))',\n",
       " 'France_sum(UnitPrice)',\n",
       " 'France_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Germany_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Germany_sum(UnitPrice)',\n",
       " 'Germany_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Greece_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Greece_sum(UnitPrice)',\n",
       " 'Greece_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Hong Kong_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Hong Kong_sum(UnitPrice)',\n",
       " 'Hong Kong_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Iceland_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Iceland_sum(UnitPrice)',\n",
       " 'Iceland_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Israel_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Israel_sum(UnitPrice)',\n",
       " 'Israel_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Italy_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Italy_sum(UnitPrice)',\n",
       " 'Italy_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Japan_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Japan_sum(UnitPrice)',\n",
       " 'Japan_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Lebanon_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Lebanon_sum(UnitPrice)',\n",
       " 'Lebanon_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Lithuania_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Lithuania_sum(UnitPrice)',\n",
       " 'Lithuania_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Malta_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Malta_sum(UnitPrice)',\n",
       " 'Malta_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Netherlands_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Netherlands_sum(UnitPrice)',\n",
       " 'Netherlands_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Norway_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Norway_sum(UnitPrice)',\n",
       " 'Norway_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Poland_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Poland_sum(UnitPrice)',\n",
       " 'Poland_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Portugal_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Portugal_sum(UnitPrice)',\n",
       " 'Portugal_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'RSA_sum(CAST(Quantity AS BIGINT))',\n",
       " 'RSA_sum(UnitPrice)',\n",
       " 'RSA_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Saudi Arabia_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Saudi Arabia_sum(UnitPrice)',\n",
       " 'Saudi Arabia_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Singapore_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Singapore_sum(UnitPrice)',\n",
       " 'Singapore_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Spain_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Spain_sum(UnitPrice)',\n",
       " 'Spain_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Sweden_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Sweden_sum(UnitPrice)',\n",
       " 'Sweden_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Switzerland_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Switzerland_sum(UnitPrice)',\n",
       " 'Switzerland_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'USA_sum(CAST(Quantity AS BIGINT))',\n",
       " 'USA_sum(UnitPrice)',\n",
       " 'USA_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'United Arab Emirates_sum(CAST(Quantity AS BIGINT))',\n",
       " 'United Arab Emirates_sum(UnitPrice)',\n",
       " 'United Arab Emirates_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'United Kingdom_sum(CAST(Quantity AS BIGINT))',\n",
       " 'United Kingdom_sum(UnitPrice)',\n",
       " 'United Kingdom_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Unspecified_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Unspecified_sum(UnitPrice)',\n",
       " 'Unspecified_sum(CAST(CustomerID AS BIGINT))']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------+\n",
      "|      date|USA_sum(CAST(Quantity AS BIGINT))|\n",
      "+----------+---------------------------------+\n",
      "|2011-12-06|                             null|\n",
      "|2011-12-09|                             null|\n",
      "|2011-12-08|                             -196|\n",
      "|2011-12-07|                             null|\n",
      "+----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date > '2011-12-05'\").select(\"date\", \"`USA_sum(CAST(Quantity AS BIGINT))`\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Aggregation Functions\n",
    "\n",
    "Only available in Scala or Java"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
